#!/usr/bin/python3
"""prometheus exporter for Ganeti cluster statistics"""

#
# Copyright (c) 2022, Wikimedia Foundation
# Copyright (c) 2023, Ganeti Project
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
# 1. Redistributions of source code must retain the above copyright notice,
# this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
# IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
# TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

__author__ = "Ganeti Project"
__version__ = "1.0.0"

import argparse
import configparser
import signal
import subprocess
import sys
import time
import logging
from typing import Iterable

import requests
import urllib3
from prometheus_client import Summary, start_http_server
from prometheus_client.core import REGISTRY, GaugeMetricFamily, Metric


class GanetiCollector():
    """
    Implements a Prometheus collector for Ganeti, using the rapi.

    ...

    Attributes
    ----------
    config : dict
        dictionary containing all parameters from the config file
    """

    # Mapping for metrics from the rapi to Prometheus metric type.
    _metric_family = {
        'ctotal': {
            'type': 'gauge',
            'desc': 'Total installed number of CPUs'
        },
        'dfree': {
            'type': 'gauge',
            'desc':'Available disk capacity'
        },
        'dtotal': {
            'type': 'gauge',
            'desc': 'Total installed disk capacity',
        },
        'mfree': {
            'type': 'gauge',
            'desc': 'Available memory capacity'
        },
        'mtotal': {
            'type': 'gauge',
            'desc': 'Total installed memory capacity'
        },
        'pinst_cnt': {
            'type': 'gauge',
            'desc': 'Number of VMs utilizing the node as primary'
        },
        'sinst_cnt': {
            'type': 'gauge',
            'desc': 'Number of VMs utilizing the node as secondary'
        },
        'oper_vcpus': {
            'type': 'gauge',
            'desc': 'Allocated number of CPUs to instance'
        },
        'oper_ram': {
            'type': 'gauge',
            'desc': 'Allocated memory to instance'
        },
    }

    # Mapping of Ganeti job states to arbitrary numbers
    _job_states = {
        'queued': 0,
        'waiting': 1,
        'canceling': 2,
        'running': 3,
        'canceled': 4,
        'success': 5,
        'error': 6
    }

    scrape_duration = Summary(
        'ganeti_scrape_duration_seconds', 'Ganeti exporter scrape duration')


    def __init__(self, config: dict):
        self.config = config
        self.uri = self.config["ganeti_api_endpoint"]
        self.auth = (self.config["ganeti_user"], self.config["ganeti_password"])
        self.cluster_info = self._gnt_request('/2/info')

        if self.config["namespace"]:
            self._prefix = f'{self.config["namespace"]}_ganeti_'
        else:
            self._prefix = 'ganeti_'


    @property
    def cluster_name(self) -> str:
        """Return name of the connected cluster"""
        return self.cluster_info['name']


    def _gnt_request(self, resource: str, bulk=False):
        uri = f'{self.uri}{resource}'

        if bulk:
            uri = f'{uri}?bulk=1'

        response = requests.get(uri, auth=self.auth,
                                verify=self.config["verify_tls"], timeout=30)
        if response.status_code != 200:
            return {}
        return response.json()


    def _run_hspace(self):
        hspace_cmd = [
            self.config["hspace_path"],
            "-m",
            self._add_auth_to_url(self.uri, self.auth[0], self.auth[1]),
            f"--disk-template={self.config['hspace_disk_template']}",
            f"--standard-alloc={self.config['hspace_alloc_data']}",
            "--machine-readable=yes",
            "--quiet"
        ]
        logging.debug('Executing %s', hspace_cmd)
        start_time = time.time()
        hspace_timeout = self.config["refresh_interval"] - 5
        try:
            command = subprocess.run(hspace_cmd, capture_output=True,
                                     check=False, timeout=hspace_timeout)
        except subprocess.TimeoutExpired:
            logging.error("Running hspace exceeded the timeout of "
                          "%d seconds, killing", hspace_timeout)
            return None
        end_time = time.time()

        logging.debug(
            "Running hspace took %d second(s)", int(end_time - start_time))

        if command.returncode == 0:
            hspace_data = {}
            for line in command.stdout.decode('utf-8').split("\n"):
                if '=' in line:
                    parts = line.strip().split('=', maxsplit=1)
                    hspace_data[parts[0]] = parts[1]
            return hspace_data

        logging.error('Failed to run %s', hspace_cmd)
        logging.error('Returncode: %d, Stdout: %s, Stderr: %s',
                      command.returncode, command.stdout, command.stderr)
        return None

    def _run_hbal(self):
        hbal_data = {}
        node_groups = self._gnt_request('/2/groups')
        for group in node_groups:
            group_name = group["name"]
            hbal_cmd = [
                self.config["hbal_path"],
                "-m",
                self._add_auth_to_url(self.uri, self.auth[0], self.auth[1]),
                "-G",
                f"{group_name}",
                self.config["hbal_extra_parameters"]
            ]
            logging.debug('Executing %s', hbal_cmd)
            start_time = time.time()
            command = subprocess.run(hbal_cmd, capture_output=True,
                                     check=False)
            end_time = time.time()

            logging.debug(
                "Running hbal took %d second(s)", int(end_time - start_time))

            if command.returncode == 0:
                for line in command.stdout.decode('utf-8').split("\n"):
                    if line.startswith("Initial score: "):
                        parts = line.split(' ')
                        initial_score = float(parts[2])
                        hbal_data[group_name] = {
                            'initial_score': initial_score,
                            'target_score': initial_score
                        }
                    if line.startswith('Cluster score improved from'):
                        parts = line.split(' ')
                        hbal_data[group_name] = {
                            'initial_score': float(parts[4]),
                            'target_score': float(parts[6])
                        }
                        break
            else:
                logging.error('Failed to run %s', hbal_cmd)
                logging.error('Returncode: %d, Stdout: %s, Stderr: %s',
                              command.returncode, command.stdout,
                              command.stderr)
                return None
        return hbal_data

    def _add_auth_to_url(self, url: str, user: str, password: str) -> str:
        url_parts = urllib3.util.parse_url(url)
        return f"{url_parts.scheme}://{user}:{password}@{url_parts.netloc}"


    def _create_gauge(self, src_type, name, labels, description='') -> (
            GaugeMetricFamily):
        prefix = self._prefix
        if not description:
            description = self._metric_family[name]['desc']
        gauge = GaugeMetricFamily(f'{prefix}{src_type}_{name}',
                                  description, labels=labels)
        return gauge


    def _create_metric(self, src_type, name, labels) -> Metric:
        metric_type = self._metric_family[name]['type']
        create_method = getattr(self, f'_create_{metric_type}')
        metric = create_method(src_type, name, labels)
        return metric


    def collect_node_metrics(self, nodes: Iterable[dict]) -> Iterable[Metric]:
        """Collect nodes metrics and return Itereable of Prometheus metrics"""
        labels = ['cluster', 'node']

        metrics = {}
        for node in nodes:
            label_values = (self.cluster_name, node['name'])
            for metric_name in node.keys():
                if metric_name in self._metric_family:
                    key = f'node_{metric_name}'
                    if not key in metrics:
                        metric = self._create_metric('node', metric_name,
                                                     labels)
                        metrics[key] = metric
                    else:
                        metric = metrics[key]
                    metric.add_metric(label_values, node[metric_name])

        return list(metrics.values())


    def collect_instance_metrics(self,
                                 instances: Iterable[dict]) -> Iterable[Metric]:
        """Collect instance metrics and return iterable of Prometheus metrics"""
        labels = ['cluster', 'instance']

        metrics = {}
        for instance in instances:
            label_values = (self.cluster_name, instance['name'])
            for metric_name in instance.keys():
                if metric_name in self._metric_family:
                    key = f'instance_{metric_name}'
                    if not key in metrics:
                        metric = self._create_metric('instance', metric_name,
                                                     labels)
                        metrics[key] = metric
                    else:
                        metric = metrics[key]
                    metric.add_metric(label_values, 0
                                      if not instance['oper_state'] else
                                      instance[metric_name])

        return list(metrics.values())


    def cpu_allocation_per_node(self, node: dict, instances: Iterable[dict],
                                primary=True) -> Metric:
        """Find the vCPUs allocated to the node. If primary is False, vCPUs
        required by secondary node is found."""
        metric_name = 'p_oper_vcpus'
        if primary:
            allocated = [instance for instance in instances
                         if instance['pnode'] == node['name']]
        else:
            metric_name = 's_oper_vcpus'
            allocated = [instance for instance in instances
                         if node['name'] in instance['snodes']]

        labels = ['cluster', 'node']

        vcpus = self._create_gauge('node', metric_name, labels,
                                   description='Total number of allocated '
                                   'vCPUs to node')
        # pylint: disable=R1728
        vcpus.add_metric((self.cluster_name, node['name']),
                         sum([instance['oper_vcpus'] for instance in allocated
                              if instance['oper_state']]))
        # pylint: enable=R1728
        return vcpus


    def collect_vcpu_allocation(self, nodes: Iterable[dict],
                                instances: Iterable[dict]) -> Iterable[Metric]:
        """Collect vCPU allocation, for nodes, primary and secondary"""
        metrics = []

        for node in nodes:
            metrics.append(self.cpu_allocation_per_node(node, instances))
            metrics.append(self.cpu_allocation_per_node(node, instances,
                                                        primary=False))

        return metrics


    def collect_summaries(self, nodes: Iterable[dict],
                          instances: Iterable[dict],
                          jobs: Iterable[dict]) -> Iterable[Metric]:
        """Create metrics based on summation of node, instance, job metrics."""
        labels = ['cluster',]
        instance_count = self._create_gauge('cluster', 'instance_count',
                        labels, description='Total number of running instances')
        instance_count.add_metric((self.cluster_name,), len(instances))

        node_count = self._create_gauge('cluster', 'node_count', labels,
                description='Total number of nodes')
        node_count.add_metric((self.cluster_name,), len(nodes))

        offline_nodes = self._create_gauge('cluster', 'offline_nodes', labels,
                description='Number of nodes offline')
        offline_nodes.add_metric((self.cluster_name,),
                                 len([node for node in nodes
                                      if node['offline']]))

        labels = ['cluster', 'job_status',]
        job_count = self._create_gauge('cluster', 'jobs', labels,
                description='Number of jobs in queue')
        for status, _ in self._job_states.items():
            job_count.add_metric((self.cluster_name, status),
                         len([job for job in jobs if job['status'] == status]))

        return [instance_count, node_count, offline_nodes, job_count]


    def collect_job_metrics(self, jobs: Iterable[dict]) -> Iterable[Metric]:
        """Create metrics based on job information"""

        labels = ['cluster', 'job_id', 'job_operation', ]
        job_wait_time = self._create_gauge('job', 'wait_time', labels,
               description='Queue wait time for jobs (seconds)')
        job_run_time = self._create_gauge('job', 'run_time', labels,
                                   description='Run time for jobs (seconds)')

        for job in jobs:
            op_id = "unknown"
            if 'ops' in job and job['ops'] is not None:
                if (job['ops'] is not None and len(job['ops']) and
                        'OP_ID' in job['ops'][0]):
                    op_id = job['ops'][0]['OP_ID']

            if job['start_ts'] is not None and job['received_ts'] is not None:
                wait_time = job['start_ts'][0] - job['received_ts'][0]
                job_wait_time.add_metric((self.cluster_name, str(job['id']),
                                          op_id), wait_time)

            if job['start_ts'] is not None and job['end_ts'] is not None:
                run_time = job['end_ts'][0] - job['start_ts'][0]
                job_run_time.add_metric((self.cluster_name, str(job['id']),
                                         op_id), run_time)

        return [job_wait_time, job_run_time]


    def _is_cluster_verify_job(self, job: dict) -> bool:
        """Check if job is a CLUSTER_VERIFY parent job"""
        summary = job.get('summary', [])
        return len(summary) > 0 and summary[0] == "CLUSTER_VERIFY"


    def _get_cluster_verify_child_job_ids(self, parent_job: dict) -> set:
        """Extract child job IDs from parent job's opresult[0]['jobs']"""
        try:
            opresult_list = parent_job.get('opresult', [])
            if not opresult_list:
                return set()

            opresult = opresult_list[0]
            if not isinstance(opresult, dict) or 'jobs' not in opresult:
                return set()

            return {
                child[1]
                for child in opresult['jobs']
                if isinstance(child, (list, tuple)) and len(child) > 1
            }
        except (IndexError, TypeError, KeyError) as e:
            logging.debug('Failed to extract child job IDs from parent %s: %s',
                         parent_job.get('id'), e)
            return set()


    def _extract_opcode(self, job: dict, default: str = "unknown") -> str:
        """Extract OP_ID from job"""
        ops = job.get('ops', [])
        if ops and isinstance(ops[0], dict) and 'OP_ID' in ops[0]:
            return ops[0]['OP_ID']
        return default


    def _convert_ganeti_timestamp(self, timestamp_tuple) -> float:
        """Convert Ganeti timestamp (seconds, microseconds) to Unix timestamp"""
        if timestamp_tuple is None:
            return None
        timestamp = float(timestamp_tuple[0])
        if len(timestamp_tuple) > 1:
            timestamp += timestamp_tuple[1] / 1_000_000.0
        return timestamp


    def _extract_cluster_verify_result(self, job: dict) -> bool:
        """Extract verify check result from job's opresult[0] (bool)"""
        opresult = job.get('opresult', [])
        if len(opresult) > 0 and isinstance(opresult[0], bool):
            return opresult[0]
        return False


    def _add_cluster_verify_job_metrics(self, cluster_verify_job: dict, parent_job_id: str,
                                         check_result: bool, job_status, job_start_time, job_end_time):
        """Add cluster-verify metrics for a job (parent or child)"""
        job_id = str(cluster_verify_job['id'])
        op_id = self._extract_opcode(cluster_verify_job, "OP_CLUSTER_VERIFY")
        check_result_str = "true" if check_result else "false"

        status = cluster_verify_job.get('status')
        if status:
            status_label_values = (self.cluster_name, job_id, op_id, status, check_result_str, parent_job_id)
            status_value = 1 if check_result else 0
            job_status.add_metric(status_label_values, status_value)

        label_values = (self.cluster_name, job_id, op_id, parent_job_id)
        start_ts = self._convert_ganeti_timestamp(cluster_verify_job.get('start_ts'))
        if start_ts is not None:
            job_start_time.add_metric(label_values, start_ts)

        end_ts = self._convert_ganeti_timestamp(cluster_verify_job.get('end_ts'))
        if end_ts is not None:
            job_end_time.add_metric(label_values, end_ts)


    def collect_cluster_verify_metrics(self, jobs: Iterable[dict]) -> Iterable[Metric]:
        """Export metrics for most recent cluster-verify job and its children"""

        status_labels = ['cluster', 'job_id', 'opcode', 'job_status', 'check_result', 'parent_job_id']
        time_labels = ['cluster', 'job_id', 'opcode', 'parent_job_id']

        job_status = self._create_gauge('cluster_verify', 'job', status_labels,
                                        description='Cluster-verify job status (1=check passed, 0=check failed)')
        job_start_time = self._create_gauge('cluster_verify', 'job_start_time', time_labels,
                                            description='Start timestamp of cluster-verify jobs (Unix time)')
        job_end_time = self._create_gauge('cluster_verify', 'job_end_time', time_labels,
                                          description='End timestamp of cluster-verify jobs (Unix time)')

        parent_job_id = None
        for job in reversed(jobs):
            if self._is_cluster_verify_job(job):
                parent_job_id = job['id']
                break

        if parent_job_id is None:
            return [job_status, job_start_time, job_end_time]

        parent_job = self._gnt_request(f'/2/jobs/{parent_job_id}')
        if not parent_job:
            logging.debug('Failed to fetch cluster-verify parent job %s', parent_job_id)
            return [job_status, job_start_time, job_end_time]

        parent_job_id_str = str(parent_job['id'])
        child_job_ids = self._get_cluster_verify_child_job_ids(parent_job)

        # Fetch full child job details (bulk query doesn't include opresult)
        child_jobs_with_results = {}
        for child_id in child_job_ids:
            child_job = self._gnt_request(f'/2/jobs/{child_id}')
            if child_job:
                check_result = self._extract_cluster_verify_result(child_job)
                child_jobs_with_results[child_id] = (child_job, check_result)

        child_check_results = [result for _, result in child_jobs_with_results.values()]
        parent_check_result = bool(child_check_results and all(child_check_results))

        logging.debug('Processing cluster-verify job %s: %d/%d children found, result=%s',
                     parent_job_id_str, len(child_jobs_with_results), len(child_job_ids),
                     parent_check_result)

        self._add_cluster_verify_job_metrics(parent_job, parent_job_id_str, parent_check_result,
                                             job_status, job_start_time, job_end_time)

        for job_id, (job, check_result) in child_jobs_with_results.items():
            self._add_cluster_verify_job_metrics(job, parent_job_id_str, check_result,
                                                 job_status, job_start_time, job_end_time)

        if len(child_jobs_with_results) < len(child_job_ids):
            missing = child_job_ids - child_jobs_with_results.keys()
            logging.debug('Cluster-verify job %s: missing %d child jobs: %s',
                         parent_job_id_str, len(missing), missing)

        return [job_status, job_start_time, job_end_time]


    def collect_hspace_metrics(self, data: dict) -> Iterable[Metric]:
        """Create metrics based on data returned by hspace"""
        labels = ['cluster', ]
        hspace_allocs = self._create_gauge('hspace',
                                           'allocatable_instances', labels,
                                           description='Allocatable instances')

        hspace_allocs.add_metric((self.cluster_name,),
                                          int(data["HTS_ALLOC_INSTANCES"]))

        return [hspace_allocs]

    def collect_hbal_metrics(self, node_groups: dict) -> Iterable[Metric]:
        """Create metrics based on data returned by hspace"""
        labels = ['cluster', 'node_group']
        hbal_initial = self._create_gauge('hbal', 'initial_score', labels,
                              description='Current hbal score per node group')
        hbal_target = self._create_gauge('hbal', 'target_score', labels,
                             description='Achievable hbal score per node group')

        for node_group, data in node_groups.items():
            hbal_initial.add_metric((self.cluster_name, node_group),
                                    data["initial_score"])
            hbal_target.add_metric((self.cluster_name, node_group),
                                   data["target_score"])

        return [hbal_initial, hbal_target]

    @scrape_duration.time()
    def collect(self) -> Iterable[Metric]:
        """Entry point for the Prometheus server to update and
        expose metrics."""
        hspace_data = None
        hbal_data = None

        nodes = self._gnt_request('/2/nodes', bulk=True)
        instances = self._gnt_request('/2/instances', bulk=True)
        jobs = self._gnt_request('/2/jobs', bulk=True)

        if self.config["hspace_enabled"]:
            hspace_data = self._run_hspace()
        if self.config["hbal_enabled"]:
            hbal_data = self._run_hbal()

        metrics = []
        metrics.extend(self.collect_node_metrics(nodes))
        metrics.extend(self.collect_instance_metrics(instances))
        metrics.extend(self.collect_summaries(nodes, instances, jobs))
        metrics.extend(self.collect_vcpu_allocation(nodes, instances))
        metrics.extend(self.collect_job_metrics(jobs))
        metrics.extend(self.collect_cluster_verify_metrics(jobs))
        if hspace_data is not None:
            metrics.extend(self.collect_hspace_metrics(hspace_data))
        if hbal_data is not None:
            metrics.extend(self.collect_hbal_metrics(hbal_data))

        return metrics


def parse_config(path: str) -> Iterable[dict]:
    config = configparser.ConfigParser()
    config_files = config.read(path)

    if not config_files:
        logging.error('Invalid configuration file')
        sys.exit(1)

    if 'ganeti' not in config.sections():
        logging.error('Unable to parse configuration, section "ganeti" not '
                      'found')
        sys.exit(1)

    required_config_keys = set(['api', 'password', 'user'])
    missing_config_keys = required_config_keys - set(config['ganeti'].keys())
    if missing_config_keys:
        logging.error('Missing configuration for: %s in ganeti section',
                      ", ".join(missing_config_keys))
        sys.exit(1)

    config_data = {
        'ganeti_api_endpoint': config['ganeti']['api'],
        'ganeti_user': config['ganeti']['user'],
        'ganeti_password': config['ganeti']['password'],
        'verify_tls': config.getboolean('default', 'verify_tls', fallback=True),
        'port': config.getint('default', 'port', fallback=8000),
        'namespace': config.get('default', 'namespace', fallback=""),
        'refresh_interval': config.getint('default', 'refresh_interval',
                                          fallback=30),
        'hspace_enabled': config.getboolean('htools', 'hspace_enabled',
                                            fallback=False),
        'hspace_path': config.get('htools', 'hspace_path',
                                  fallback='/usr/bin/hspace'),
        'hspace_disk_template': config.get('htools', 'hspace_disk_template',
                                           fallback='plain'),
        'hspace_alloc_data': config.get('htools', 'hspace_alloc_data',
                                        fallback='20480,2048,2'),
        'hbal_enabled': config.getboolean('htools', 'hbal_enabled',
                                          fallback=False),
        'hbal_path': config.get('htools', 'hbal_path',
                                fallback='/usr/bin/hbal'),
        'hbal_extra_parameters': config.get('htools', 'hbal_extra_parameters',
                                            fallback='')
    }

    logging.debug('Loaded configuration: %s', config_data)
    if config_data["hbal_enabled"] or config_data["hspace_enabled"]:
        logging.warning("Data collections via htools enabled - please raise "
                        "prometheus scrape_timeout to at least 20 seconds. "
                        "Depending on the cluster size, executing the tools "
                        "may easily hit the default timeout of 10 seconds.")

    return config_data


# pylint: disable=unused-argument
def handle_sigterm(sig, frame):
    logging.info("Received SIGTERM, terminating")
    sys.exit(0)
# pylint: enable=unused-argument


def main():
    parser = argparse.ArgumentParser(
        description=f'Prometheus Exporter for Ganeti, Version {__version__}')

    parser.add_argument('--config',
                        default='/etc/ganeti/prometheus.ini',
                        help='path to configuration file.')
    parser.add_argument('--loglevel',
                        default='warning',
                        choices=['error', 'warning', 'info', 'debug'],
                        help='set the loglevel (debug level may print '
                             'sensitive data to the console!)')
    parser.add_argument('--version',
                        action='store_true',
                        help='print version number and exit')

    args = parser.parse_args()

    if args.version:
        print(__version__)
        sys.exit(0)

    if not args.config:
        parser.error('No config file provided')

    if args.loglevel == 'error':
        loglevel = logging.ERROR
    elif args.loglevel == 'warning':
        loglevel = logging.WARNING
    elif args.loglevel == 'debug':
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO
    logging.basicConfig(format='%(levelname)s: %(message)s', level=loglevel)
    logging.info('Starting prometheus-ganeti-exporter version %s', __version__)
    logging.info("Loglevel set to %s",
                 logging.getLevelName(logging.getLogger().getEffectiveLevel()))

    signal.signal(signal.SIGTERM, handle_sigterm)

    config = parse_config(args.config)

    # If TLS verfication has been disabled, don't spam the logs with warnings
    # that we're already well aware of.
    if not config["verify_tls"]:
        logging.warning("Disabling TLS verification as requested by "
                        "configuration")
        urllib3.disable_warnings()

    logging.info('Initializing Ganeti Collector')
    c = GanetiCollector(config)
    REGISTRY.register(c)

    logging.info('Starting HTTP server on 0.0.0.0:%d,', config["port"])
    start_http_server(config["port"])

    try:
        while True:
            time.sleep(config["refresh_interval"])
    except KeyboardInterrupt:
        sys.exit(1)


if __name__ == '__main__':
    main()
